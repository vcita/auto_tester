# Build Rules - Exploring and Generating Tests

## When to Use This

Use these rules when asked to "build", "explore", or "generate" a test or function from a `steps.md` file.

## Prerequisites

Before building, verify:
1. The target has a `steps.md` file with clear objectives and steps
2. You have access to the Playwright MCP browser tools (navigate, snapshot, click, type, etc.)
3. You know the target URL (from config.yaml or steps.md)

## CRITICAL: Use Playwright MCP for Exploration and Debugging

**ALWAYS use the Playwright MCP server for browser exploration**, not the built-in cursor-ide-browser.

**When debugging failing tests: NEVER guess what the issue is. ALWAYS run the test using MCP and observe the UI as a user would. Trying to fix and run again is never the right way - check what doesn't work and try to understand by observing the UI first.**

### Why Playwright MCP:
- **Full accessibility tree capture** - sees dropdown menus, modal content, nested elements
- **Proper iframe handling** - automatically navigates into iframes
- **Better element identification** - captures text content in custom components
- **Same API as test code** - locators discovered during exploration work directly in test.py

### What Playwright MCP Solves:
The built-in cursor-ide-browser has limitations:
- Cannot see dropdown menu items (shows empty `role: menu`)
- Misses text content in custom UI components
- Struggles with nested iframes

### Configuration:
Playwright MCP is configured in `.cursor/mcp.json`:
```json
{
  "mcpServers": {
    "playwright": {
      "command": "npx",
      "args": ["-y", "@playwright/mcp@latest"]
    }
  }
}
```

### Exploration Workflow:
1. Playwright MCP opens a **separate browser window** (not embedded panel)
2. You may need to manually solve CAPTCHAs (Cloudflare, reCAPTCHA)
3. Take snapshots to see the full accessibility tree
4. Interact with elements using the discovered refs
5. Record the Playwright locators that work (shown in tool output)

## CRITICAL: Research Phase - Knowledge Center

**BEFORE exploring or writing any test**, consult the vcita Knowledge Center to understand the feature:

### How to Research:
1. **Search online** for the relevant vcita help article (site:support.vcita.com)
2. **Identify the standard user flow** documented by vcita
3. **Note required vs optional fields** in forms
4. **Understand success criteria** from the documentation

### Why This Matters:
- Ensures tests follow the expected user journey
- Identifies correct button labels and menu names
- Reveals edge cases mentioned in documentation
- Prevents building tests based on assumptions

### Example Research:
```
For "Add New Client" test:
1. Search: "vcita add client site:support.vcita.com"
2. Find article: "Add Clients and Contacts in vcita"
3. Learn: Click Clients → New Contact → Fill details → Save
4. Note: A "contact" becomes a "client" after first transaction
```

### Knowledge Center URL:
https://support.vcita.com/hc/en-us

## CRITICAL: Use URLs from config.yaml

**ALWAYS use URLs defined in `config.yaml` when accessing vcita.**

### Required URLs from config:
- **Login**: Use `target.login_url` (https://www.vcita.com/login) - NOT app.vcita.com/login
- **Base URL**: Use `target.base_url` for URL verification patterns

### Why:
- `www.vcita.com/login` avoids Cloudflare security checks that block automation
- `app.vcita.com/login` triggers Cloudflare challenges requiring manual intervention
- Consistent URL usage ensures tests work reliably

### In MCP exploration:
```python
# WRONG - triggers Cloudflare
page.goto("https://app.vcita.com/login")

# RIGHT - uses configured login URL
page.goto("https://www.vcita.com/login")
```

## CRITICAL: Real User Actions Only

Tests must simulate how a real, average user interacts with the application.

### ALWAYS:
- Click visible buttons and links to navigate
- Use menus and navigation elements
- Fill forms through the UI
- Wait for pages to load naturally

### NEVER:
- Navigate directly to internal URLs (e.g., `/users/logout`, `/app/settings`)
- Use "hidden" or "shortcut" URLs that users wouldn't know
- Bypass UI flows with direct navigation
- Assume knowledge of URL patterns

## CRITICAL: No Fallbacks or Alternative Flows

**Tests must have ONE deterministic flow. No fallbacks, no alternatives.**

### Why:
- Tests are verification tools, not resilient scripts
- If the primary flow fails, it's either a **bug** (report it) or the **test is wrong** (fix it)
- Fallbacks hide real issues and make tests unreliable
- Each test should verify ONE specific user journey

### NEVER include in scripts:
- "If X doesn't work, try Y"
- "Alternative flow" sections
- Try/except blocks that silently continue
- Multiple ways to achieve the same step

### If exploration reveals the primary flow doesn't work:
1. **Stop and investigate** - why doesn't it work?
2. **Is it a bug?** - document it and report
3. **Is the test wrong?** - update `steps.md` with the correct flow
4. **Never** add a fallback to work around the issue

## CRITICAL: Report Suspected Bugs - Do Not Bypass

**If you encounter behavior that appears to be a bug or malfunctioning feature, STOP immediately and notify the user.**

### What counts as a suspected bug:
- Feature doesn't work as documented/expected
- Button/action does nothing or produces unexpected results
- Data doesn't save correctly
- UI elements missing or broken
- Error messages that shouldn't appear
- Inconsistent behavior (works sometimes, fails others)
- Workarounds needed for basic functionality

### When you suspect a bug:
1. **STOP** - Do not continue building/exploring
2. **Document what you observed** - exact steps, expected vs actual behavior
3. **Take a screenshot** if possible
4. **Notify the user immediately** - describe the issue clearly
5. **Wait for guidance** - the user will decide how to proceed

### Why this matters:
- Bugs should be reported and fixed, not worked around
- Silent workarounds hide real issues from the development team
- Tests that bypass bugs give false confidence
- The user needs to know about system issues

### NEVER do this:
- Find a workaround and continue without mentioning the bug
- Assume "it's supposed to work this way"
- Add code to handle buggy behavior silently
- Complete the test by avoiding the broken feature

### Example:
```
WRONG: "The email field doesn't save, so I'll skip it and just fill the required fields"
RIGHT: "STOP - I noticed the email field value disappears after typing. This appears to be a bug. 
        Steps to reproduce: 1) Open client form 2) Type in email field 3) Click elsewhere - value disappears.
        Should I report this or is this expected behavior?"
```

### Example - WRONG:
```markdown
## Alternative Flow (via Sidebar)
If Quick Actions modal doesn't work:
1. Click "Properties" in sidebar
2. Click "Add property" button
```

### Example - RIGHT:
```markdown
## Steps
1. Click "Quick Actions" button
2. Click "Add property" in menu
   - If this fails: STOP - investigate and fix
```

## CRITICAL: Investigate Failures In The Same Flow

**If a field doesn't save or an action doesn't work, DO NOT move on. Investigate immediately.**

### When something doesn't work during exploration:
1. **Try the same action again** in the same flow
2. **Take a snapshot** to see the current state
3. **Try different approaches** to the SAME action (different locator, click first, type slowly)
4. **Understand WHY** it failed before proceeding
5. **Document the root cause** and the solution

### NEVER do this:
- Complete the task with missing fields
- Skip a step that didn't work
- Move to a "different flow" to work around the issue
- Assume "it probably saved" without verifying

### Example - WRONG:
```
1. Fill email field → value disappears
2. Fill phone field → value disappears  
3. Click Save → "Property created successfully"
4. Move on to next test (email and phone are empty!)
```

### Example - RIGHT:
```
1. Fill email field → value disappears
2. STOP - why did it disappear?
3. Try again: click field first, then type
4. Still doesn't work? Try typing slowly with press_sequentially()
5. Value retained! Document the solution
6. Apply same fix to phone field
7. Verify ALL fields before clicking Save
8. After save, verify ALL fields were actually saved
```

### This applies to:
- Form fields that don't retain values
- Buttons that don't respond to clicks
- Dropdowns that don't open
- Any action that doesn't produce expected results

### Only Exception:
Direct navigation is ONLY allowed to the application's main entry points that users would bookmark or type:
- Login page: `https://www.vcita.com/login`
- Public marketing pages

### Example:
```
WRONG (shortcut):     page.goto("https://www.vcita.com/users/logout")
RIGHT (user action):  Click avatar button -> Click "Logout" menu item

WRONG (shortcut):     page.goto("https://app.vcita.com/app/calendar")  
RIGHT (user action):  Click "Calendar" in sidebar navigation

WRONG (shortcut):     page.goto(f"https://app.vcita.com/app/clients/{id}")
RIGHT (sequential):   Verify URL already contains the ID (from previous test)
RIGHT (user action):  Search for record name, click it in results
```

### Sequential Tests - Browser State Continuity
Tests in a sequence share browser state. Each test's end state becomes the next test's start state.

**Pattern for sequential tests:**
```python
# WRONG - unconditionally navigates via URL
page.goto(f"https://app.vcita.com/app/clients/{matter_id}")

# RIGHT - verifies expected state from previous test
if matter_id not in page.url:
    raise ValueError(f"Expected to be on matter page {matter_id}, but URL is {page.url}")
page.wait_for_load_state("domcontentloaded")
```

**Why this matters:**
- If the previous test failed to leave browser in correct state, we want to KNOW about it
- Silent navigation hides issues in the test chain
- Ensures tests actually verify the full user journey

## Build Process Overview

```
steps.md (human intent)
    ↓ [You explore with browser]
    ↓ [Record successful actions]
script.md (detailed actions)
    ↓ [Generate code]
test.py (executable)
    ↓ [Update changelog]
changelog.md (audit trail)
```

## Step-by-Step Build Process

### 1. Read and Understand steps.md

- Read the Objective section to understand the goal
- Read the Steps section to understand the flow
- Note any Parameters (for functions) or Test Data
- Identify the starting URL

### 2. Check Existing Functions

BEFORE exploring, check `tests/_functions/_functions.yaml`:
- Can any existing function handle part of this test?
- If yes, plan to use `Call: function_name` instead of re-implementing

### 3. Explore with Playwright MCP Browser

For each step in steps.md, follow the **Locator Decision Process**:

#### a) Take a snapshot to see the current page state
```
Use: browser_snapshot
```
The snapshot shows the accessibility tree with `ref` attributes for each element.

#### b) Identify the target element and ALL possible locators

From the snapshot, identify:
- The element you need to interact with
- **ALL possible ways to locate it** (this is critical!)

Document the options:
| Option | Description |
|--------|-------------|
| `get_by_role("button", name="Save")` | Role-based, most resilient |
| `get_by_text("Save")` | Text-based |
| `locator(".btn-save")` | CSS class |
| `get_by_role("button").nth(2)` | Position-based |

#### c) Evaluate each locator option (REQUIRED)

For EACH option, consider:

| Criteria | Question |
|----------|----------|
| **Uniqueness** | Does it match exactly ONE element? Check the snapshot! |
| **Resilience** | Will it survive if classes/text/order changes? |
| **Readability** | Is it clear what element this targets? |
| **Iframe context** | Does it need frame_locator() nesting? |

Create a decision table:
| Option | Pros | Cons | Verdict |
|--------|------|------|---------|
| `get_by_role("button", name="Save")` | Unique, semantic | None | **BEST** |
| `locator(".btn-save")` | Clear intent | Class might change | Fallback |
| `get_by_role("button").nth(2)` | Works | Fragile if order changes | Avoid |

#### d) Choose the best locator

Pick based on this priority:
1. **Unique role + accessible name** (most resilient)
2. **Unique label** (for form fields)
3. **Unique text content** (readable)
4. **Test-id/data attribute** (if available and stable)
5. **CSS class** (if semantic and unlikely to change)
6. **Position-based (.nth())** (LAST RESORT - document why)

#### e) Validate your choice in MCP

**Before documenting**, test the chosen locator:
```
Use: browser_click, browser_type, etc.
```
- Pass the `ref` value and a human-readable `element` description
- The tool output shows the **Playwright code** that was executed
- **Confirm the expected behavior occurred**

If it fails, try your next option from the decision table.

#### f) Record the VERIFIED Playwright code

**CRITICAL**: The tool output includes `### Ran Playwright code`
```
### Ran Playwright code
await page.locator('iframe[title="angularjs"]').contentFrame()
    .locator('#vue_iframe_layout').contentFrame()
    .getByRole('button', { name: 'Save' }).click();
```

This verified code goes directly into script.md - **do not modify it**.

#### g) Document in script.md

For each step, record:
- **LOCATOR DECISION**: The options you evaluated (table)
- **CHOSEN**: Which option and why
- **VERIFIED PLAYWRIGHT CODE**: The exact code from MCP (converted to Python)
- **How verified**: What behavior confirmed it worked
- **Fallback locators**: Alternatives for self-healing

### Handling Iframes

vcita uses nested iframes. Playwright MCP handles this automatically, showing refs like `f47e18` (iframe refs start with `f`).

The tool output shows how to access elements in iframes:
```python
# Single iframe
page.locator('iframe[title="angularjs"]').contentFrame().get_by_role(...)

# Nested iframes
page.locator('iframe[title="angularjs"]').contentFrame()
    .locator('#vue_iframe_layout').contentFrame().get_by_text(...)
```

### Handling Dropdowns

Dropdowns that don't work with the built-in browser work perfectly with Playwright MCP:
1. Click the dropdown to open it: `browser_click` on the listbox
2. Take a snapshot - you'll see the options with their refs
3. Click the desired option: `browser_click` on the option ref

### CRITICAL: Handling Virtual Scrolling / Lazy-Loaded Lists (Endless Scroll)

**If you cannot find an expected element in a list, table, or grid, ALWAYS scroll down MULTIPLE TIMES until no more items load before concluding it doesn't exist.**

#### Why this matters:
- Many modern UIs use **virtual scrolling** or **endless scroll** - items below the viewport are NOT rendered in the DOM
- Items may exist in the data but are invisible to Playwright until scrolled into view
- A locator search will return 0 results even if the item exists but is off-screen
- **Endless scroll lists require multiple scrolls** - each scroll loads a new batch of items
- This is especially common in long lists (services, clients, appointments, etc.)

#### Symptoms that suggest virtual/endless scrolling:
- You create/add an item, but `getByText()` or `filter(has_text=)` returns 0 matches
- The list shows many items but the snapshot only shows a subset
- Items at the top of a list are visible, but newer items at the bottom are not
- After navigating away and back, recently created items are "missing"
- You scroll once but still can't find the item - need to scroll multiple times

#### Solution - Scroll MULTIPLE TIMES until end of list:
```python
# WRONG - may fail due to endless scroll (only scrolls once)
add_button = iframe.get_by_role('button', name='Add 1 on 1 Appointment')
add_button.scroll_into_view_if_needed()
page.wait_for_timeout(500)
service_in_list = iframe.get_by_role('button').filter(has_text=name)
service_in_list.wait_for(state='visible', timeout=10000)  # MAY STILL TIMEOUT!

# RIGHT - scroll multiple times until no more items load
# Step 1: Wait for list to load
iframe.get_by_text("My Services").wait_for(state="visible", timeout=10000)

# Step 2: Scroll to bottom multiple times until no new items appear
max_scrolls = 10
previous_count = 0
for scroll_attempt in range(max_scrolls):
    # Get all service buttons currently visible
    all_services = iframe.get_by_role("button").filter(has_text="Test Consultation")
    current_count = all_services.count()
    
    if current_count > 0:
        # Scroll the last visible service into view to trigger loading more
        last_service = all_services.nth(current_count - 1)
        last_service.scroll_into_view_if_needed()
        page.wait_for_timeout(1000)  # Wait for new items to load
        
        # Check if new items loaded
        new_count = all_services.count()
        if new_count == previous_count:
            # No new items loaded - we've reached the end
            break
        previous_count = new_count
    else:
        # No services found yet, scroll to "Add" button to trigger initial load
        add_button = iframe.get_by_role('button', name='Add 1 on 1 Appointment')
        add_button.scroll_into_view_if_needed()
        page.wait_for_timeout(1000)

# Step 3: NOW search for the specific item
service_in_list = iframe.get_by_role('button').filter(has_text=name)
service_in_list.wait_for(state='visible', timeout=10000)  # SUCCESS!
```

#### How to scroll in MCP exploration:
```javascript
// Scroll multiple times until no more items load
let previousCount = 0;
for (let i = 0; i < 10; i++) {
  const services = await iframe.locator('button').filter({ hasText: /Test Consultation/ });
  const count = await services.count();
  if (count > 0) {
    const lastService = services.nth(count - 1);
    await lastService.scrollIntoViewIfNeeded();
    await page.waitForTimeout(1000);
    const newCount = await services.count();
    if (newCount === previousCount) break; // No new items
    previousCount = newCount;
  }
}
```

#### When to apply this pattern:
- **ALWAYS** after creating a new item in a list (services, clients, events, etc.)
- When verifying an item exists in a long list
- When searching for an item that was created in a previous test
- When navigating back to a list page after being elsewhere
- **ESPECIALLY** when the list uses endless/infinite scroll (scrolls multiple times needed)

#### Key insight:
**Think like a user** - if a user wanted to find an item at the bottom of a long list with endless scroll, they would scroll down MULTIPLE TIMES until they reach the end. Your test must do the same - scroll repeatedly until no more items load, THEN search.

## CRITICAL: Always Use press_sequentially() for Text Input

**ALWAYS use `.press_sequentially()` instead of `.fill()` for typing text into fields.**

### Why:
- **Better simulation of real users** - real users type character by character
- **Triggers all input events** - some fields rely on keydown/keyup events
- **Works with autocomplete fields** - comboboxes and autocomplete inputs require this
- **More reliable** - avoids issues with fields that don't accept bulk input

### Standard pattern for ALL text input:
```python
# ALWAYS use this pattern:
field.click()  # Focus the field first
page.wait_for_timeout(100)  # Brief wait for any transformations
field.press_sequentially("value to type")  # Type character by character
```

### Example:
```python
# WRONG - may not work with all fields
iframe.get_by_role("textbox", name="Email").fill("test@example.com")

# RIGHT - works reliably with all fields
email_field = iframe.get_by_role("textbox", name="Email")
email_field.click()
page.wait_for_timeout(100)
email_field.press_sequentially("test@example.com")
```

### Special case - Autocomplete/Combobox fields:
Some fields transform from `textbox` to `combobox` when clicked:
```python
# Click the textbox (transforms to combobox)
iframe.get_by_role("textbox", name="Email").click()
page.wait_for_timeout(100)
# Type into the combobox
iframe.get_by_role("combobox", name="Email").press_sequentially("test@example.com")
```

### When to detect autocomplete fields:
- Field shows as `textbox` initially but value disappears after typing
- Field changes role from `textbox` to `combobox` when clicked
- If unsure, always use click + press_sequentially pattern

### 4. Generate script.md

After successful exploration, create `script.md` with the **new verified code format**:

```markdown
# [Test Name] - Detailed Script

> **Status**: Generated from exploration
> **Last Updated**: [current date/time]
> **Generated By**: Cursor AI exploration with Playwright MCP

## Initial State
- URL: [starting URL]
- Prerequisites: [any setup needed]

## Actions

### Step 1: [Step description]
- **Action**: [Click/Type/Navigate/etc.]
- **Target**: [Human description of element]

**LOCATOR DECISION:**

| Option | Pros | Cons |
|--------|------|------|
| `get_by_role("button", name="Save")` | Unique, semantic | None |
| `locator(".btn-save")` | Clear class | Less resilient |

**CHOSEN**: `get_by_role("button", name="Save")` - Unique and semantic.

**VERIFIED PLAYWRIGHT CODE**:
```python
save_button = page.get_by_role("button", name="Save")
save_button.click()
```

- **How verified**: Clicked in MCP, form submitted successfully
- **Value**: [if typing, the value]
- **Wait for**: [what indicates success]
- **Fallback locators**: `.btn-save`, `button:has-text("Save")`

### Step 2: [Next step]
...

## Success Verification
- [How to verify the test passed]
```

**CRITICAL**: Every step MUST have:
- **LOCATOR DECISION** table showing options evaluated
- **CHOSEN** with rationale
- **VERIFIED PLAYWRIGHT CODE** that was tested in MCP
- **How verified** describing what confirmed it worked

### 5. Generate test.py

Convert script.md to Playwright Python code following the rules in `phase3_code.mdc`.

**CRITICAL**: Copy the VERIFIED PLAYWRIGHT CODE directly from script.md.

Key points:
- **Copy VERIFIED PLAYWRIGHT CODE exactly** - do not modify or "improve" it
- The locator decision was already made - don't re-decide
- Add comments mapping to script.md steps
- Include the standard header

**NEVER do this:**
```python
# script.md has: inner_iframe.get_by_role("button").nth(2)
# DO NOT "improve" it to:
inner_iframe.locator(".edit-button")  # WRONG - this wasn't tested!
```

**ALWAYS do this:**
```python
# Copy exactly from script.md VERIFIED PLAYWRIGHT CODE:
inner_iframe.get_by_role("button").nth(2)  # RIGHT - this was tested!
```

### 6. Update changelog.md

Add an entry:
```markdown
## [date/time] - Initial Build
**Phase**: All files
**Author**: Cursor AI (exploration)
**Reason**: Built from steps.md via browser exploration
**Changes**:
- Generated script.md from exploration
- Generated test.py from script.md
```

## Handling Function Calls

If steps.md contains `Call: function_name`:

1. Find the function in `tests/_functions/[function_name]/`
2. Read its `steps.md` to understand what it does
3. In script.md, document it as:
   ```markdown
   ### Step N: [Description]
   - **Action**: Call function
   - **Function**: [function_name]
   - **Parameters**:
     - param1: "value1"
     - param2: "value2"
   ```
4. In test.py, import and call the function

## Handling Failures During Exploration

If an action fails during exploration:

1. **Take a screenshot** to capture the current state
2. **Re-examine the page** - maybe the element has a different selector
3. **Try alternative approaches** - different selector, different action
4. **If stuck**, document what you tried and ask for help

## Example: Building the Login Function

Given `tests/_functions/login/steps.md`:
```markdown
# Login to vcita

## Steps
1. Navigate to the login page
2. Enter the username: {username}
3. Enter the password: {password}
4. Click the login button
5. Wait for dashboard to load
```

Exploration process:
1. Navigate to login URL
2. Snapshot - find email input (get_by_label("Email"), input[type="email"])
3. Type username - verify it appears in field
4. Snapshot - find password input
5. Type password
6. Snapshot - find login button (get_by_role("button", name="Sign in"))
7. Click button
8. Wait for URL change to dashboard
9. Snapshot - verify dashboard elements visible

Then generate script.md and test.py from these findings.

## CRITICAL: Complete the Full Action During Exploration

**ALWAYS complete the full action during exploration. NEVER stop before the final step.**

### Why:
- You cannot know the full experience without completing the action
- There may be confirmation dialogs, success messages, or unexpected screens
- Validation requirements can only be discovered by seeing the end result
- The UI state after completion may differ from expectations

### During exploration, ALWAYS:
1. **Complete every action to its final state** - if deleting, actually delete
2. **See the confirmation/result screen** - what does the system show after?
3. **Verify the action took effect** - check the actual data changed
4. **Document the complete flow** - including any post-action screens

### NEVER:
- Stop at a confirmation dialog without clicking through
- Assume what happens after an action
- Skip the final step to "preserve data"
- Write test validation based on assumptions

### Example - WRONG:
```
1. Select client checkbox
2. Click More → Delete
3. See confirmation dialog
4. Click Cancel (to preserve the client)
5. Write test assuming Delete works
```

### Example - RIGHT:
```
1. Select client checkbox
2. Click More → Delete
3. See confirmation dialog "Delete properties?"
4. Click Delete to confirm
5. See: Client removed from list, count decreased
6. Search for client name - not found
7. NOW write the test with accurate validation
```

### If you need test data:
- Create temporary test data first (e.g., create a client to delete)
- Complete the full action on that test data
- Document exactly what happens at each step

## CRITICAL: Run the Test Before Marking Complete

**You are NEVER done building a test until you run it and see it succeed.**

### Why:
- Code that looks correct may have subtle bugs
- Locators that worked during exploration may need adjustment
- Timing issues only appear during actual execution
- A test that doesn't run is not a test

### Required steps before marking a test complete:
1. **Save all test files** (steps.md, script.md, test.py, changelog.md)
2. **Run the test** using the standalone `if __name__ == "__main__":` block
3. **Watch the browser** - verify each step executes correctly
4. **See the test pass** - all assertions must succeed
5. **Fix any failures** - iterate until the test passes
6. **Document the successful run** in changelog.md

### How to run a test:
```bash
cd C:\Programming\vcita_tester
python tests/clients/delete_matter/test.py
```

### If the test fails:
1. **Read the error message** - what specifically failed?
2. **ALWAYS read the screenshot** - A screenshot is ALWAYS captured on failure in `snapshots/screenshots/`. This is the most important debugging artifact - it shows exactly what the browser looked like when the failure occurred.
3. **Analyze screenshot vs expected** - What SHOULD be visible? What IS visible?
4. **Debug the locator** - maybe the element changed
5. **Fix the test.py** - update locators, add waits, etc.
6. **Run again** - repeat until it passes

**CRITICAL: Never skip step 2 (reading the screenshot). It reveals issues that error messages alone cannot.**
6. **Update script.md** - if you learned something new about the flow

## CRITICAL: Validate Success from User Perspective (NO TOAST MESSAGES)

**Every test MUST verify the action succeeded by checking the ACTUAL RESULT in the system. NEVER rely on toast messages or confirmation dialogs.**

### Why:
- **Toast messages are unreliable** - they can appear even when actions fail, or may not appear at all
- **Confirmation dialogs don't guarantee completion** - backend could still fail
- **The only reliable validation is seeing the actual change** - data in the UI proves success
- **Users validate success by seeing results** - tests should do the same

### ALWAYS validate by:
1. **Checking the actual data exists/changed** - is the new item visible? is the deleted item gone?
2. **Verifying the UI reflects the change** - list updated, form shows new values
3. **Searching/navigating to confirm** - find the created item, verify deleted item is gone
4. **Checking counts/totals** - did the number increase/decrease?

### NEVER validate by (these are FORBIDDEN):
- Toast messages ("Success!", "Deleted successfully", "Created!", "Appointment scheduled")
- Confirmation dialogs closing
- Alert messages appearing
- "Saved" or "Updated" text appearing temporarily
- Snackbar notifications
- Any transient UI element that auto-dismisses

### What counts as "actual data":
- Items appearing in lists/grids
- Items appearing in calendar views
- URL containing the created item's ID
- Page title containing item name
- Form fields showing saved values
- Search results showing the item

### Validation Examples:

#### Delete Test - WRONG:
```python
# Click delete, see toast "Deleted successfully"
expect(page.get_by_text("Deleted successfully")).to_be_visible()  # NOT ENOUGH!
```

#### Delete Test - RIGHT:
```python
# After delete, verify the item is GONE from the list
expect(page.get_by_role("row", name=matter_name)).to_have_count(0)

# Optionally, search for it to double-confirm
search_box.fill(matter_name)
expect(page.get_by_text("No results found")).to_be_visible()
```

#### Create Test - WRONG:
```python
# Click save, see toast "Created successfully"
expect(page.get_by_text("Created")).to_be_visible()  # NOT ENOUGH!
```

#### Create Test - RIGHT:
```python
# After save, verify the item EXISTS with correct data
expect(page).to_have_url(re.compile(r"/app/clients/[a-z0-9]+"))
expect(page.get_by_text(created_name)).to_be_visible()
expect(page.get_by_text(created_email)).to_be_visible()
```

#### Update Test - WRONG:
```python
# Click save, dialog closes
expect(dialog).not_to_be_visible()  # NOT ENOUGH!
```

#### Update Test - RIGHT:
```python
# After save, verify the NEW value is shown (not the old one)
expect(page.get_by_text(new_value)).to_be_visible()
expect(page.get_by_text(old_value)).not_to_be_visible()
```

### The validation rule:
**If a user wouldn't trust the action completed without checking, neither should the test.**

## Quality Checklist

Before marking build complete:
- [ ] **LOCATOR DECISION documented** for every interactive step
- [ ] **VERIFIED PLAYWRIGHT CODE** included for every step (tested in MCP)
- [ ] script.md has fallback locators per step (for self-healing)
- [ ] test.py **copies VERIFIED PLAYWRIGHT CODE exactly** - no modifications
- [ ] test.py follows the code structure from phase3_code.mdc
- [ ] changelog.md is updated
- [ ] **EXPLORATION WAS COMPLETED FULLY** - all actions done to final state
- [ ] **TEST WAS RUN AND PASSED** - not just written, actually executed
- [ ] **VALIDATION CHECKS ACTUAL RESULTS** - not just toasts/confirmations
