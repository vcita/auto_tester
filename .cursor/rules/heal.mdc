# Heal Rules - Fixing Failed Tests

## When to Use This

Use these rules when processing a heal request from `.cursor/heal_requests/` or when asked to "fix", "heal", or "repair" a failing test.

## Heal Request Location

Heal requests are stored in: `.cursor/heal_requests/`

Filename format: `heal_[test_id]_[timestamp].md`

## Heal Request Format

```markdown
# Heal Request: [test/function name]

## Type
heal

## Target
[path to test folder, e.g., tests/_functions/login/]

## Error
[The error message from the test failure]

## Failed At
[Which step failed, if known]

## Screenshot (REQUIRED)
[Path to failure screenshot - ALWAYS captured automatically on failure]
**YOU MUST READ AND ANALYZE THIS IMAGE before proceeding with any fix!**

## DOM Snapshot
[Path to DOM snapshot at failure, if available]

## Current script.md
[Full content of the current script.md]

## Current test.py
[Full content of the current test.py]

## Instructions
Please analyze the failure and fix the test.
```

## MANDATORY FIRST STEP: Read the Changelog

**BEFORE attempting any fix, you MUST read the test's changelog.md:**

1. **Locate the changelog** - `tests/{category}/{test_name}/changelog.md`
2. **Read the FULL history** - Understand what has been tried before
3. **Note previous failures and fixes** - What locators/approaches were already attempted?
4. **Identify patterns** - Has this same issue occurred before? What fixed it?

**WHY THIS IS CRITICAL:**
- Prevents repeating failed approaches
- Shows what locators were tried and didn't work
- Reveals if the issue is recurring (might be a deeper problem)
- Saves time by learning from past attempts

**Example changelog insight:**
```markdown
## 2026-01-20 - Healed (Selector Update)
**Error**: "Element not found: .edit-button"
**Fix**: Changed to get_by_role("button").nth(2) - CSS class doesn't exist

## 2026-01-19 - Healed (Selector Update)  
**Error**: "Element not found: button[aria-label='Edit']"
**Fix**: Tried .edit-button - that didn't work either
```

From this you learn: Don't try `.edit-button` or `button[aria-label='Edit']` - they were already tried and failed!

## MANDATORY SECOND STEP: Screenshot AND Video Analysis

**AFTER reading the changelog, analyze the visual evidence:**

1. **Locate the screenshot** - Path is in the heal request under "## Screenshot"
2. **Read/view the screenshot** - Use the Read tool to view the image
3. **Check for video recording** - Path is in heal request under "## Video Recording"
   - Video shows the FULL test execution, not just the end state
   - Watch for repeated actions, timing issues, or unexpected behavior
4. **Analyze what you see** - Document:
   - What is visible on screen?
   - Did the expected UI element appear?
   - Is there an error dialog?
   - Is the page in the expected state?
5. **Compare expected vs actual** - Based on the failing step, what SHOULD be visible vs what IS visible?

**The screenshot shows the END state. The video shows the JOURNEY - it reveals timing issues, repeated actions, and the exact moment of failure.**

If the heal request doesn't have a screenshot, that's a bug - the test runner should always capture one on failure.

## MANDATORY THIRD STEP: Debug with MCP (NO BLIND FIXES!)

**CRITICAL RULE: When a test fails, NEVER guess what the issue is. ALWAYS run it using MCP and observe the UI as a user would.**

### Use the SAME account as the failed test (CRITICAL)

**You MUST use the exact same account (same credentials) that ran the failing test when exploring with MCP.** The heal request and config.yaml (e.g. `target.auth`) contain the username (and optionally password) that the runner used. Log in with those credentials in the MCP browser before reproducing the test steps.

- **If the MCP browser is already logged in** (e.g. dashboard loads when you open the app), **do not assume it is the right account.** Check the visible user/email. If it is not the account from the heal request (e.g. "MCP Welcome Test" vs `itzik+autotest.1769462440@vcita.com`), **log out, then log in** with the credentials from the heal request or `config.yaml`.
- **Using a different account invalidates the exploration:** Different accounts can have different data (e.g. no clients vs. a list of matters), different verticals ("Properties" vs "Clients"), different UI state, and different permissions. You may see a different bug or no bug at all, and any fix you propose may not apply to the account that actually failed.
- **Before starting the test steps in MCP:** Confirm you are on the same account (e.g. user menu or page shows the username from the heal request). Only then run the failing test flow step-by-step.

### The Wrong Way (NEVER DO THIS):
- ❌ Look at error message and guess the fix
- ❌ Try a different selector based on assumptions
- ❌ Make code changes and run again hoping it works
- ❌ Fix based on screenshots alone without understanding the flow

### The Right Way (ALWAYS DO THIS):
- ✅ **Use the SAME account as the failed test** - Log in with the credentials from the heal request or config.yaml (`target.auth`). If MCP is already logged in as another user, log out first, then log in with the failed test's account. Do not explore while logged in as a different user.
- ✅ **Open MCP browser** - Navigate to the starting point of the failing test (after confirming correct account)
- ✅ **Execute test steps one by one** - Run each action exactly as the test.py code does
- ✅ **Observe the UI like a user** - Watch what actually happens on screen
- ✅ **Verify each step visually** - Take snapshot after EVERY action to confirm it worked
- ✅ **Find the EXACT failure point** - Which specific action fails? What happens visually?
- ✅ **Understand WHY it fails** - Is it a selector issue? Timing? Element state? UI change?
- ✅ **Only THEN write the fix** - Based on what you learned from MCP debugging

**Key Principle: Trying to fix and run again is NEVER the right way. Check what doesn't work and try to understand by observing the UI first.**

### What MCP Debugging Reveals:
The screenshot shows the END state. MCP debugging shows you the JOURNEY to that state and reveals:
- Which step actually failed (not just where the error occurred)
- What the element structure looks like at that moment
- What selectors would work (by testing them live)
- What timing issues exist (by watching the UI respond)
- What the user actually sees (not what the code assumes)

**CRITICAL: Do NOT write code fixes based on guessing from screenshots or error messages alone!**

You must SEE what happens in the browser, step by step, to understand the real problem.

### When debugging: account not ready (first-time setup)

If you see the **"Welcome to vcita!"** dialog (Phone *, Business size *, Continue) or "Just a few basic questions" after login, the **account has not completed first-time setup**. Tests assume the account is past onboarding; they will fail (e.g. "Could not find form frame with 'First Name'") because the add-matter/add-client form never opens. This is not a test bug—fix by ensuring the account completes onboarding: run `python main.py create_user` and confirm it finishes (including validation), or complete the Welcome flow once manually for the config account. See context.md "Test Account Prerequisites".

### MCP Uses Its Own Browser (Cannot Attach to Python-Launched Browser)

Playwright MCP starts and controls **its own** browser. The AI **cannot** attach MCP to a browser that was opened by the Python test runner or a debug script. Therefore:

- **Do NOT** run `--until-test` expecting the AI to "drive that browser with MCP" — MCP cannot attach to the runner's browser. If you (the human) want to debug in the open window, the runner **leaves the browser open**; press Enter in the terminal when done to close it.
- **Do** run `python main.py run --category <cat> --until-test "<Test Name>"` to get the runner to **dump context** to **until_test_context.json** in the run dir (with `next_test`, `url`, `title`, `context`; password redacted) and leave the browser open. To debug **with MCP**, start a **new** MCP browser, log in (base_url + "/login", config credentials), and use the dumped **url** and **context** to run the failing test steps in MCP.

### MCP Debugging: Availability and Config

1. **If you cannot find or use the MCP, halt and alert.** Do not attempt to debug the test without Playwright MCP. If MCP tools are unavailable or not responding, stop the healing flow, tell the user that MCP is required for debugging, and do not apply blind fixes.

2. **You MUST use the same account as the failed test when exploring with MCP.** This is not optional. When debugging with MCP:
   - **Log in with the exact credentials** from the heal request or config (e.g. `target.auth` in config.yaml). The heal request often includes `username` or login_url; use that account.
   - **If the MCP browser is already in a session** (e.g. you navigate to the app and see a dashboard), **verify it is the same user** that ran the failed test. If you see a different name/email (e.g. "MCP Welcome Test" or another test account), **log out and log in** with the failed test's credentials before reproducing any steps.
   - **Use the same context** the failing test had (e.g. matter names, IDs from the heal request or `until_test_context.json`) so list data and state match.
   - **Why this matters:** Debugging on a different account or with different context can hide the real issue (e.g. empty list vs. list with rows, different vertical labels, different UI or permissions). Fixes based on the wrong account may not work when the test runs again with the configured account.

## Heal Process Overview

```
Heal Request (failure context)
    ↓ [1. FIRST: Read changelog.md - learn from past attempts!]
    ↓ [2. SECOND: Read and analyze the screenshot!]
    ↓ [3. THIRD: Debug step-by-step with MCP - NO BLIND FIXES!]
    ↓ [4. Analyze the error message with MCP findings]
    ↓ [5. Determine: selector issue vs flow change vs product bug]
    ↓ [6. Write fix based on MCP-verified understanding (avoid past failures)]
script.md (updated)
    ↓ [Regenerate code]
test.py (fixed)
    ↓ [Update changelog]
    ↓ [Delete heal request]
```

## Step-by-Step Heal Process

### 0. Read the Changelog (MANDATORY FIRST)

**Before anything else**, read `tests/{category}/{test_name}/changelog.md`:

1. Look for previous heal attempts on this test
2. Note what locators/approaches were tried and failed
3. Note what fixes worked in the past (might need similar approach)
4. Check if this is a recurring issue

**If the changelog shows this exact issue was fixed before and is now failing again:**
- The previous fix might have been fragile
- Consider a more robust approach this time
- Don't just repeat the same fix

### 1. Analyze the Failure

Read the heal request and determine the failure type:

| Error Pattern | Likely Cause | Action |
|---------------|--------------|--------|
| "Element not found" | Selector changed | Re-explore to find new selector |
| "Timeout waiting for" | Page structure changed | Re-explore the flow |
| "Expected X but got Y" | Logic/data issue | Check if product bug |
| "Navigation failed" | URL changed | Update URL in script |

### 2. Classify the Issue

**A) Selector Issue (Most Common)**
- Element exists but selector is outdated
- Fix: Find new selector, update script.md and test.py

**B) Flow Change**
- The UI flow has changed (new steps, different order)
- Fix: Re-explore entire flow, regenerate script.md

**C) Product Bug**
- The feature itself is broken
- Action: Report bug, mark test as blocked, skip healing

### 3. Step-by-Step Debugging with MCP (REQUIRED)

**MANDATORY: When a test fails, you MUST use MCP to observe what actually happens. Do NOT try to guess the fix.**

When debugging a failing test, you MUST use the Playwright MCP browser to execute the test step-by-step, exactly as the test code does. This is the ONLY reliable way to identify what's broken.

**CRITICAL RULES:**

1. **NEVER guess the issue** - Always observe the UI first using MCP
2. **Think like a user** - What would a user see? What would they experience?
3. **Execute EXACTLY like the test code** - Use the same selectors, same actions, same order as in test.py
4. **Validate EVERY step visually** - After each action, take a screenshot or snapshot to verify:
   - Did the action succeed?
   - Is the UI in the expected state?
   - Are the values filled correctly?
5. **Never assume success** - Just because no error occurred doesn't mean it worked
6. **Compare actual vs expected** - After filling a field, verify the value is actually there
7. **Stop immediately on failure** - When a step doesn't produce the expected result, investigate before continuing
8. **Understand before fixing** - Don't try to fix and run again. Understand what's wrong first by observing the UI

**BEFORE STARTING: Create Step Checklist (MANDATORY)**

Before executing any actions in the browser, you MUST:

1. Read the test.py file completely
2. Extract ALL action steps (clicks, types, selects, waits, verifications)
3. Create a numbered checklist with ALL steps
4. Track completion as you go: `[ ]` pending, `[x]` completed, `[!]` failed

Example checklist format:
```
Test: create_matter (15 steps total)
[ ] Step 1: Click Quick Actions button
[ ] Step 2: Click Add property
[ ] Step 3: Click Show more
[ ] Step 4: Fill First Name
[ ] Step 5: Fill Last Name
[ ] Step 6: Fill Email
[ ] Step 7: Fill Mobile phone
[ ] Step 8: Fill Address
[ ] Step 9: Fill Referred by
[ ] Step 10: Fill Property address
[ ] Step 11: Fill Help request
[ ] Step 12: Select Property type dropdown
[ ] Step 13: Fill Special instructions
[ ] Step 14: Fill Private notes
[ ] Step 15: Click Save
```

**Checklist Rules:**
- **NEVER skip ahead** - Complete steps in sequential order
- **NEVER mark complete** until visually verified with screenshot
- **Count progress** - Always know "Step X of Y" as you work
- **Update checklist** after each step with status and any findings

This ensures no steps are accidentally skipped (like dropdown selections or optional fields).

**Step-by-Step Debugging Process:**

```
1. (Optional) Run with --until-test to dump context; then load until_test_context.json for url and context.
2. Open browser with MCP (MCP's own browser). **Log in with the SAME account as the failed test:**
   - Go to base_url + "/login" (from config). If the browser is already logged in, check the visible user/email.
   - If it is NOT the account from the heal request / config.yaml (target.auth), log out, then log in with that account's credentials.
   - Do not proceed to reproduce the test until you have confirmed you are on the correct account.
3. Navigate to the failing test's starting URL (from until_test_context.json or from test flow); use context values as needed.
4. Take snapshot to see current state
5. For EACH step in test.py:
   a. Execute the action using MCP (click, type, etc.)
   b. Take snapshot/screenshot IMMEDIATELY after
   c. VERIFY the action worked:
      - For clicks: Did the expected UI appear?
      - For typing: Is the text actually in the field?
      - For selections: Is the correct option selected?
   d. If verification fails: STOP and investigate
   e. If verification passes: Continue to next step
6. Document all findings - which steps work, which fail
7. **CRITICAL: Complete the ENTIRE flow with MCP before updating code** - Don't update test code until you've successfully run the whole flow with MCP and verified it works end-to-end
8. Only AFTER full validation, update the test code
```

**Example: Debugging a Form Fill**

```
Step in test.py:
  email_field = iframe.get_by_role("textbox", name="Email")
  email_field.fill("test@example.com")

MCP Debugging:
  1. browser_type to fill the email field
  2. browser_snapshot immediately after
  3. CHECK: Does the snapshot show "test@example.com" in the Email field?
     - YES: Step works, continue
     - NO: Step failed! The field might be a combobox, have autocomplete,
           or need click first. Investigate before continuing.
```

**Common Issues Found During Step-by-Step Debugging:**

| Symptom | Likely Cause | Solution |
|---------|--------------|----------|
| Field appears empty after fill() | Autocomplete/combobox field | Use pressSequentially() or click first |
| Wrong element clicked | Multiple elements match selector | Make selector more specific |
| Action works but UI doesn't update | Async loading | Add wait_for after action |
| Value is truncated | Input validation/mask | Check field constraints |

### 3.5. Key UI Interaction Patterns (CRITICAL)

**These patterns were learned from real debugging sessions and MUST be applied when interacting with UI elements:**

1. **Hover before interacting with hidden buttons**: If buttons only appear on hover (common in list items, table rows, etc.), you MUST hover over the parent element first, then wait for buttons to appear before finding/clicking them.
   ```python
   # WRONG - buttons won't be visible
   menu_button.click()
   
   # RIGHT - hover first to reveal buttons
   parent_element.hover()
   page.wait_for_timeout(1500)  # Wait for buttons to appear
   menu_button.click()
   ```

2. **Use MCP to inspect DOM during interaction**: When debugging complex UI interactions, use MCP to inspect the actual DOM structure while hovering/interacting, rather than guessing from code. The DOM structure may reveal classes, hierarchy, or states that aren't obvious from the code.

3. **Use multiple class checks for button identification**: When multiple similar buttons exist (e.g., 3-dots menu vs payment button), require multiple classes (e.g., both "three-dots" AND "activator-container") and explicitly exclude unwanted classes (e.g., "take-payment-button").
   ```python
   # WRONG - might match wrong button
   button = container.locator('button.three-dots')
   
   # RIGHT - requires both classes and excludes unwanted
   button = container.locator('button.three-dots.activator-container:not(.take-payment-button)')
   # Or check in code:
   has_three_dots = "three-dots" in btn_class
   has_activator = "activator-container" in btn_class
   has_take_payment = "take-payment-button" in btn_class
   if has_three_dots and has_activator and not has_take_payment:
       # This is the correct button
   ```

4. **Check for confirmation dialogs after menu actions**: After clicking menu items, a confirmation dialog may appear in a different iframe context (outer vs inner); handle it explicitly.
   ```python
   # After clicking menu item
   menu_item.click()
   page.wait_for_timeout(2000)  # Wait for dialog
   
   # Check outer iframe first (where dialogs often appear)
   submit_btn = outer_iframe.get_by_role('button', name='Submit')
   if submit_btn.count() > 0:
       submit_btn.first().click()
   ```

5. **Use evaluate() for reliable clicking**: If Playwright's click() fails on visible elements, use `element.evaluate()` with `scrollIntoView()` and `click()` for more reliable interaction.
   ```python
   # If normal click fails
   try:
       button.click()
   except:
       # Use evaluate for force-click
       button.evaluate("""
           (el) => {
               el.scrollIntoView({ behavior: 'instant', block: 'center' });
               el.click();
           }
       """)
   ```

6. **CRITICAL: Don't update test code until MCP flow succeeds**: Complete the entire flow step-by-step with MCP before updating test code. Only code after the flow works in MCP. This ensures you understand the actual UI behavior and have verified the fix works end-to-end.

### 4. Re-Explore (If Needed)

For selector issues:
1. Navigate to the page where failure occurred
2. Take a snapshot
3. Find the element that should be there
4. Note new selectors that work
5. Test the action with new selector

For flow changes:
1. Start from the beginning of the test
2. Follow the steps.md goals (not the old script.md)
3. Record the new flow
4. Generate new script.md

### 4. Update script.md

For selector fixes:
```markdown
### Step 3: Click Submit Button
- **Action**: Click
- **Target**: Submit button
- **Element hints**:
  - `get_by_role("button", name="Submit")` [NEW - was "Save"]
  - `.submit-btn` [REMOVED - no longer exists]
  - `button[type="submit"]`
```

For flow changes:
- Regenerate the entire Actions section
- Keep the same structure as build.mdc specifies

### 5. Regenerate test.py

Update the code to use new selectors:
```python
# Step 3: Click Submit Button
# HEALED: Changed from "Save" to "Submit" button
page.get_by_role("button", name="Submit").click()
```

### 6. Update changelog.md

```markdown
## [date/time] - Healed (Selector Update)
**Phase**: script.md, test.py
**Author**: Cursor AI (heal)
**Reason**: Element selector changed
**Error**: "Element not found: button:has-text('Save')"
**Fix**: Updated to use "Submit" button instead of "Save"
**Changes**:
- Updated Step 3 selector in script.md
- Regenerated test.py with new selector
```

### 7. Verify the Fix

After updating:
1. The test should be re-run by the test runner
2. If it passes, healing is complete
3. If it fails again, a new heal request will be created

### 8. Delete the Heal Request

Once fixed, delete the heal request file from `.cursor/heal_requests/`

## Escalation: Standalone Debug Script When Stuck

When a test remains unfixed after **5 MCP-focused attempts** (analyze with MCP, apply fix, run category; repeat), do not only mark the heal request as UNRESOLVED. **Create a dedicated standalone Python debug script** to accelerate debugging.

**Trigger:** After 5 attempts without resolving the test.

**Steps:**

1. **Copy the skeleton**  
   Copy `debug_test_skeleton.py` (project root) to `debug_<category>_<test_name>.py`.  
   Naming: use underscores, e.g. `debug_events_remove_attendee.py`.

2. **Configure**  
   Set `TARGET_URL` (and any IDs from context or heal request) to the test’s starting page.  
   Copy the failing test’s steps from `test.py` into the skeleton’s STEP_2 … STEP_N blocks.

3. **Focus on the failure**  
   Put the **failing action or assertion** in the `DEBUG_FOCUS` section. Add extra `print` calls or try multiple locators/variants to see what works.

4. **Run the script**  
   From project root: `python debug_<category>_<test_name>.py`.  
   The skeleton uses the same browser/context as the runner, so behaviour matches the full run.

5. **Use the findings**  
   Document what you observed (which step fails, what the UI shows, any variant that worked). Use this to decide the next fix or to document UNRESOLVED with a clearer "estimation of what might be the issue."

Then complete the UNRESOLVED documentation and status update as described in the heal command.

## Handling Product Bugs

If you determine the failure is a product bug (not a test issue):

1. **Do NOT try to fix the test** - the test is correct, the product is broken

2. **Create a bug report** in `.cursor/bug_reports/`:
   ```markdown
   # Bug Report: [brief description]
   
   ## Test
   [path to test]
   
   ## Expected Behavior
   [what should happen]
   
   ## Actual Behavior
   [what actually happens]
   
   ## Evidence
   - Screenshot: [path]
   - Error: [message]
   
   ## Discovered
   [date/time]
   ```

3. **Mark test as blocked** - update `_category.yaml`:
   ```yaml
   - id: test_name
     status: blocked
     blocked_reason: "Bug: [brief description]"
     blocked_since: [date]
   ```

4. **Delete the heal request** - it's been processed

## Cascade Updates

If healing reveals the flow changed significantly:

1. Update `script.md` with new flow
2. Regenerate `test.py`
3. Consider if `steps.md` needs updating (if goals changed)
4. Log all changes in `changelog.md`

## Quality Checklist

Before marking heal complete:
- [ ] **Changelog was read first** - learned from past attempts
- [ ] **Fix doesn't repeat past failures** - not using approaches that already failed
- [ ] Root cause identified and documented
- [ ] script.md updated with VERIFIED PLAYWRIGHT CODE (tested in MCP)
- [ ] test.py regenerated (copied verified code exactly)
- [ ] changelog.md updated with heal entry (including what was tried)
- [ ] Heal request deleted
- [ ] (If bug) Bug report created and test marked blocked
